# 05线性代数

![image-20231208123851480](https://s2.loli.net/2023/12/08/WRwMYeFdpP2Ogho.png)

## 5.1线性代数

**标量**

- 简单操作
  $$
  c = a + b \\
  c = a * b \\
  c = sina
  $$
  

- 长度
  $$
  \begin{equation}
  \left| a \right| = 
  \begin{cases}
  a & \text{ $ if a > 0 $ } \\
  -a & \text{ $ otherwise $ }
  \end{cases}
  \end{equation}
  $$

  $$
  \left| a + b\right| <= \left| a \right| + \left| b \right|  \\
  \left| a * b\right| = \left| a \right|  *  \left| b \right|
  $$

**向量**

- 简单操作
  $$
  ||a||_2 = \left[ \sum_{i=1}^m a_i^2 \right]^{1/2} \\
  ||a|| >= 0 for all a \\
  ||a + b|| <= ||a|| + ||b|| \\
  ||a * b|| = |a| * ||b||
  $$
  

- 长度

【举例】

$c = a + b$

![image-20231208221234555](https://s2.loli.net/2023/12/08/FJW457BjzIxQhAU.png)

$c = a * b$

![image-20231208221304693](https://s2.loli.net/2023/12/08/iHdQYVvpjbSwI4R.png)

- 点乘

$$
a^T b = \sum_i a_ib_i
$$



- 正交

$$
a^Tb = \sum_i a_i b_i = 0
$$

![image-20231208221644639](https://s2.loli.net/2023/12/08/TStW7HOB1r4J9XF.png)



**矩阵**

- 简单操作

$$
C = A + B \\
C = a * B \\
C = sinA
$$

- 乘法（矩阵乘以向量）

$$
c = Ab
$$

![矩阵乘以向量](https://s2.loli.net/2023/12/08/aFrORcwp1KMbfhP.png)



**直观上理解矩阵乘法**：向量可以看作空间，矩阵可以看作是特定的工具，向量与特定的矩阵相乘后，将会使得向量变得扭曲，也就是变换到另一个空间中

![直观上理解矩阵乘法](https://s2.loli.net/2023/12/08/GEneLpHOWuBFza5.png)



- 范数

$$
c = A * b \\
hence |c| <= ||A|| * ||b||
$$

- 取决于如何衡量 b 和 c的长度
- 常见的范数
- 矩阵范数：最小的满足的上面公式的值
- Frobenius 范数

$$
||A||_{Frob} = \left[ \sum_{ij} A_{ij}^2 \right]^{1/2}
$$

**特殊矩阵**

- 对称和反对称

$$
A_{ij} = A_{ji} \\
A_{ij} = -A_{ji}
$$

![image-20231208223352977](https://s2.loli.net/2023/12/08/WkO897cMeLTEVqG.png)

- 正定

$$
||x||^2 = x^Tx >= 0 \\
generalizes to \\
x^TAx >= 0
$$

![image-20231208223522068](https://s2.loli.net/2023/12/08/eQ3w7MXjvZBKmEl.png)

- 正交矩阵
  - 所有行都相互正交
  - 所有行都有单位长度 $u$
  - 可以写成
- 置换矩阵
- 置换矩阵是正交矩阵
- 特征向量和特征值
  - 不被矩阵改变方向的向量$x$（**但长度可能会改变**）

$$
Ax = \lambda x
$$

![image-20231208224252581](https://s2.loli.net/2023/12/08/aqi97HUjBVQGk1F.png)

- 对称矩阵总是可以找到特征向量



## 5.2线性代数实现

标量由只有一个元素的张量表示

![image-20231209152426875](https://s2.loli.net/2023/12/09/SGmW8QPnMXzfIor.png)



你可以将向量视为标量值组成的列表

![image-20231209152539663](https://s2.loli.net/2023/12/09/GmoESUCyKabAnXV.png)

通过张量的索引来访问任一元素

![image-20231209152604605](https://s2.loli.net/2023/12/09/WeOzi8kUPvHhZDt.png)



访问张量的长度

![image-20231209152703768](https://s2.loli.net/2023/12/09/fB4mHSrdutLOPJw.png)



只有一个轴的张量，形状只有一个元素

![image-20231209152726497](https://s2.loli.net/2023/12/09/3r4JBczaENgw9Gd.png)



通过指定两个分量 $m$ 和 $n$ 来创建一个形状 $m * n$ 的矩阵

![image-20231209152855536](https://s2.loli.net/2023/12/09/aDjNKtfdH6vXb9g.png)



矩阵的转置

![image-20231209152938567](https://s2.loli.net/2023/12/09/6VOAfsBmClFR4Dt.png)



对称矩阵（symmetric matrix）$A$ 等于其转置：$A = A^T$

![image-20231209153046682](https://s2.loli.net/2023/12/09/ryJBHx4iSZCNaoQ.png)



![image-20231209153111807](https://s2.loli.net/2023/12/09/B6XvFPcsIaHyEwr.png)



就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构 **代表2个具有3 * 4结构的矩阵的组合**

![image-20231209153215038](https://s2.loli.net/2023/12/09/z2JnxGuyglNSBX6.png)



给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量

![image-20231209153448715](https://s2.loli.net/2023/12/09/9xKdV6USFkj1yHq.png)



![image-20231209153501138](https://s2.loli.net/2023/12/09/z4UXC9R8aLsDGKQ.png)



两个矩阵的按元素的成绩称为 哈达玛积（Hadamard product）（数学符号）⊙

![image-20231209153835936](https://s2.loli.net/2023/12/09/cTdrO89U1tLs3Ny.png)



![image-20231209153850528](https://s2.loli.net/2023/12/09/71WP9ROlLvsFifZ.png)



计算其元素的和



![image-20231209153932420](https://s2.loli.net/2023/12/09/N1CK9gH4YFcJD5m.png)



表示任意形状张量的元素和



![image-20231209154014998](https://s2.loli.net/2023/12/09/vRTyCNFwMLeUurW.png)



指定求和汇总张量的轴



axis=0 意味着对 2 这一维度进行求和，剩下的两个维度留下来了。



![image-20231209154251883](https://s2.loli.net/2023/12/09/O5mESaK81ujvLzg.png)

![image-20231209154222362](https://s2.loli.net/2023/12/09/SBplV19n6WCbJ8U.png)



axis=1 意味着对 5 这一维度进行求和，剩下的两个维度留下来了。



![image-20231209154529821](https://s2.loli.net/2023/12/09/f3FbEpYLR1dtPga.png)



也可以指定其中的多个维度进行求和，其余的维度留下来



![image-20231209154709279](https://s2.loli.net/2023/12/09/LfJOmsx1oaqD28k.png)



![image-20231209154755666](https://s2.loli.net/2023/12/09/ypw5d9il6NRGrc2.png)



**下面解释一下是怎么按特定轴求和的**

```
原始的tensor.size([2, 5, 4]), 一共是40个数
tensor([[[ 0,  1,  3,  4],
		 [ 5,  6,  7,  8],
		 [ 9, 10, 11, 12],
		 [13, 14, 15, 16],
		 [17, 18, 19, 20]],
		 
		[[21, 22, 23, 24],
		 [24, 25, 26, 27],
		 [28, 29, 30, 31],
		 [32, 33, 34, 35],
		 [36, 37, 38, 39]]])
按 axis=0 求和，也就是
```

一个与求和相关的量是 平均值（mean 或 average）

![image-20231209162139075](https://s2.loli.net/2023/12/09/qYuWCpZ1zIGEOlm.png)



计算总和或均值时保持轴数不变

![image-20231209162248301](https://s2.loli.net/2023/12/09/ktsEXCSLOF1phTP.png)



通过广播将 $A$ 除以 sum_A 



![image-20231209162431838](https://s2.loli.net/2023/12/09/sJ9u4axNmzAZeOj.png)

某个轴计算 $A$ 元素的累积总和



![image-20231209162526414](https://s2.loli.net/2023/12/09/4qhyUSHTos8bQpE.png)



点积是相同位置的按元素乘积的和4



![image-20231209162702754](https://s2.loli.net/2023/12/09/PitU9EjMDT6rswy.png)



我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积



![image-20231209162847916](https://s2.loli.net/2023/12/09/l82JiN9IY45mAz7.png)



矩阵向量积 $Ax$ 是一个长度为 $m$ 的列向量，其 $i^{th}$ 元素是点积 $a_i^Tx$



![image-20231209163033727](https://s2.loli.net/2023/12/09/O8qkeSBwUpjYFCs.png)



我们可以将矩阵-矩阵乘法 $AB$ 看作是简单地执行 $m$ 次矩阵-向量积，并将结果拼接在一起，形成一个 $n * m$ 的矩阵



![image-20231209163240282](https://s2.loli.net/2023/12/09/WgdQNoFIHRqrk8E.png)



$L_2$ 范数是向量元素平方和的平方根
$$
||x||_2 = \sqrt{\sum_{i-1}^n x_i^2}
$$
![image-20231209163429546](https://s2.loli.net/2023/12/09/RX5k97ySiZHUJ8D.png)



$L_1$ 范数，它表示为向量元素的绝对值之和：
$$
||x||_1 = \sum_{i=1}^n |x_i|
$$
![image-20231209163603565](https://s2.loli.net/2023/12/09/nIaBMVCW8K6OlNT.png)



矩阵的 *弗罗贝尼乌斯范数* （Frobenius norm） 是矩阵元素的平方和的平方根：


$$
||X||_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}
$$
![image-20231209163941729](https://s2.loli.net/2023/12/09/9JXOxPIDnt1ZmUR.png)

## 5.3按特定轴求和

![image-20231209164342978](https://s2.loli.net/2023/12/09/zfXoBY1nAbhLOUF.png)

## 5.4QA

